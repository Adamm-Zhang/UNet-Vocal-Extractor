# -*- coding: utf-8 -*-
"""torch_audio_test_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pytB7OzV9aQv1Rt_v0l_zDzVC8dfnpXn
"""

!pip install musdb stempeg

import musdb
import torch

mus = musdb.DB(download=True, subsets='train', root='musdb18hq')

import os
import ffmpeg
import torchaudio
import numpy as np

n_fft = 2048
hop_length = 512

def wav2spec(wav):
    wavMono = wav.mean(-1)
    window = torch.hann_window(n_fft, device=wav.device)

    spec = torch.stft(wavMono, n_fft=n_fft, hop_length=hop_length, return_complex=True, center=True, window=window)
    return spec


def load_stem(path):  # remember to get phase too later
  streams = {}
  phase = {}

  for streamIndex, name in enumerate(['mix', 'vocals']):
    out_stream = (
        ffmpeg
        .input(path)
        .output('pipe:', format='f32le', ac=2, ar=44100, map=f'0:a:{streamIndex}')
    )

    out, err = ffmpeg.run(out_stream, capture_stdout=True, capture_stderr=True)
    audio = np.frombuffer(out, np.float32).reshape(-1, 2)

    spectrum = wav2spec(torch.from_numpy(audio))
    magnitude = spectrum.abs()

    streams[name] = magnitude / (magnitude.max() + 1e-8)
    phase[name] = torch.angle(spectrum)

  return [streams, phase]

stems = load_stem("/content/musdb18hq/train/ANiMAL - Clinic A.stem.mp4")
spectrum_magnitude = stems[0]['mix']

import matplotlib.pyplot as plt
plt.imshow(20*torch.log10(stems[0]["mix"].abs()+1e-6).numpy(), origin='lower', aspect='auto')
plt.xlabel('Time frames')
plt.ylabel('Frequency bins')

from torch.utils.data import IterableDataset
from torch.utils.data import DataLoader

class MUSDBStreamDataset(IterableDataset):
    def __init__(self, root_dir, split="train"):
        self.root_dir = root_dir
        self.split_dir = os.path.join(root_dir, split)
        self.track_files = [
            os.path.join(self.split_dir, f)
            for f in os.listdir(self.split_dir)
            if f.endswith(".stem.mp4")
            ]
    def __iter__(self):
        for item in self.track_files:
            stems = load_stem(item)

            yield stems


# each batch contains [mix, vocals] as spectrographs
loader = DataLoader(MUSDBStreamDataset("/content/musdb18hq"), batch_size=1)
for i, batch in enumerate(loader):
#   #pass
  if i < 5:
    print(batch[0].keys())
    print(batch[0]['vocals'].shape)
    print(batch[0]['mix'].shape)
  else:
    break

import torch.nn as nn

class doubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
      super().__init__()
      self.conv = nn.Sequential(
          nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
          nn.BatchNorm2d(out_channels),
          nn.ReLU(inplace=True),
          nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
          nn.BatchNorm2d(out_channels),
          nn.ReLU(inplace=True)
      )

    def forward(self, x):
      return self.conv(x)


class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, features=[16, 32, 64, 128]):
        super().__init__()
        self.encoder = nn.ModuleList()
        self.decoder = nn.ModuleList()

        # Encoder
        for f in features:
            self.encoder.append(
                doubleConv(in_channels, f)
            )
            in_channels = f

        self.bottleneck = nn.Sequential(
            doubleConv(features[-1], features[-1] * 2)
        )

        # Decoder
        for f in reversed(features):
            self.decoder.append(
                nn.Sequential(
                    nn.ConvTranspose2d(f * 2, f, kernel_size=2, stride=2),  # upsample
                    nn.BatchNorm2d(f),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(f * 2, f, kernel_size=3, padding=1),  # after concat
                    nn.BatchNorm2d(f),
                    nn.ReLU(inplace=True)
                )
            )
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)

    def forward(self, x):
        skips = []
        for enc in self.encoder:
            x = enc(x)
            skips.append(x)
            x = nn.functional.max_pool2d(x, 2)

        x = self.bottleneck(x)
        for dec in self.decoder:    # we slice decode layer operations 0:3 and 3:0 so we can concat the skip layer in between!
            x = dec[0:3](x)  # upsample part // note that this is in for dec in self.decoder - it is the current operation for number of features. i.e. conv > BN > RELU
            #print(dec[0:3])
            skip = skips.pop()
            x = nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)
            x = torch.cat((x, skip), dim=1)
            x = dec[3:](x)

        return self.final_conv(x)

x = torch.randn(1, 1, 1025, 587)
model = UNet(in_channels=1, out_channels=1)
y = model(x)
print(y.shape)
print(x.shape)

from torch.cuda.amp import autocast, GradScaler
import time

device = 'cuda' if torch.cuda.is_available() else 'cpu'

model = UNet(in_channels=1, out_channels=1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
scaler = GradScaler()
criterion = torch.nn.L1Loss()

train_loader1 = DataLoader(MUSDBStreamDataset("/content/musdb18hq"), batch_size=1)
#for batch in train_loader1:
 # print(batch['mix'])

for epoch in range(50):
    model.train()
    running_loss = 0.0

    for i, stems in enumerate(train_loader1):
        # Extract only vocals from stems
        # stems might be a list of spectrograms or a stacked tensor
        # vocal = stems[0] if isinstance(stems, (list, tuple)) else stems

        mix = stems[0]['mix'].unsqueeze(1)
        vocal = stems[0]['vocals'].unsqueeze(1)
        # Move to GPU
        mix = mix.to(device)     # shape (B, 1, F, T)
        vocal = vocal.to(device) # shape (B, 1, F, T)

        # Forward
        optimizer.zero_grad()
        with torch.amp.autocast('cuda'):
            pred_mask = torch.sigmoid(model(mix))
            pred_vocal = pred_mask * mix
            loss = criterion(pred_vocal, vocal)

        # Backprop
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()

        if i % 50 == 0:
            print(f"[{epoch:02d}/{i}] loss={loss.item():.4f}")

    print(f"Epoch {epoch}: avg_loss={running_loss/(i+1):.4f}")

torch.save(model.state_dict(), "/content/thisModel/UNET.pth")

import soundfile as sf

test_loader = DataLoader(MUSDBStreamDataset("/content/musdb18hq", split="test"), batch_size=1)

model.eval()
for i, stems in enumerate(train_loader1):
    # Extract only vocals from stems
    # stems might be a list of spectrograms or a stacked tensor
    # vocal = stems[0] if isinstance(stems, (list, tuple)) else stems

    mix = stems[0]['mix'].unsqueeze(1)
    vocal = stems[0]['vocals'].unsqueeze(1)
    # Move to GPU
    mix = mix.to(device)     # shape (B, 1, F, T)
    vocal = vocal.to(device) # shape (B, 1, F, T)

    vocalPhase = stems[1]['vocals'].to(device)
    mixPhase = stems[1]['mix'].to(device)

    # Forward
    sample_rate = 44100
    with torch.inference_mode():
        pred = model(mix)
        pred_vocal_mag = pred * mix
        pred_vocal_mag *= (pred_vocal_mag.max() + 1e-8)

        vocal_complex = pred_vocal_mag * torch.exp(1j * mixPhase)
        vocal_waveform = torch.istft(vocal_complex.squeeze(1), n_fft=n_fft, hop_length=hop_length, window=torch.hann_window(n_fft, device=vocal_complex.device), center=True)
        sf.write(f"/content/predictions/pred_vocals{i}.wav",
                  vocal_waveform.cpu().T.numpy(),  # soundfile expects [frames, channels]
                  sample_rate)

        #mag = torch.sqrt((pred_vocal_mag.squeeze(1) ** 2).sum())
        #print(mag)